{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "160fe6b6",
   "metadata": {},
   "source": [
    "# Controlling a Multi-Agent Team\n",
    "\n",
    "This notebook covers how to control the lifecycle of a multi-agent team, including resetting, stopping, resuming, and aborting the team.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec7e24cb-7049-4da1-9dca-aa119c575978",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from rich import print as rprint\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.conditions import TextMentionTermination\n",
    "from autogen_agentchat.teams import RoundRobinGroupChat\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "\n",
    "# Initialize the model client\n",
    "model_client = OpenAIChatCompletionClient(\n",
    "    model=\"gpt-4o-2024-08-06\",\n",
    ")\n",
    "\n",
    "# Define the agents with distinct roles\n",
    "researcher = AssistantAgent(\n",
    "    \"researcher\",\n",
    "    model_client=model_client,\n",
    "    system_message=\"You are a diligent researcher who gathers relevant information.\",\n",
    ")\n",
    "\n",
    "analyst = AssistantAgent(\n",
    "    \"analyst\",\n",
    "    model_client=model_client,\n",
    "    system_message=\"You analyze information and provide insightful summaries.\",\n",
    ")\n",
    "\n",
    "writer = AssistantAgent(\n",
    "    \"writer\",\n",
    "    model_client=model_client,\n",
    "    system_message=\"You write clear and concise reports based on analysis. Signal completion by including the word 'DONE'.\",\n",
    ")\n",
    "\n",
    "# Define termination condition\n",
    "termination_condition = TextMentionTermination(\"DONE\")\n",
    "\n",
    "# Create the team\n",
    "team = RoundRobinGroupChat(\n",
    "    [researcher, analyst, writer],\n",
    "    termination_condition=termination_condition,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca56f98-35aa-4d26-a2de-4de0fd4a1df0",
   "metadata": {},
   "source": [
    "## Resetting the Team\n",
    "\n",
    "Resetting clears the team's state and prepares it for a new task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c069cc63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Team has been reset.\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "from autogen_agentchat.conditions import ExternalTermination\n",
    "\n",
    "async def reset_team():\n",
    "    await team.reset()\n",
    "    print(\"Team has been reset.\")\n",
    "\n",
    "## NOTE: we are running asynchronously\n",
    "asyncio.run(reset_team())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700626b8",
   "metadata": {},
   "source": [
    "## Stopping the Team Externally\n",
    "\n",
    "You can stop the team from outside by using an external termination condition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b0072f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Team stopped externally.\n"
     ]
    }
   ],
   "source": [
    "async def stop_team_externally():\n",
    "    external_termination = ExternalTermination()\n",
    "    team.termination_condition = external_termination\n",
    "\n",
    "    run_task = asyncio.create_task(team.run(task=\"Write a short story about teamwork.\"))\n",
    "    await asyncio.sleep(0.2)  # Let the team run briefly\n",
    "    external_termination.set()  # Signal the team to stop\n",
    "    await run_task\n",
    "    print(\"Team stopped externally.\")\n",
    "\n",
    "# To execute, run in an async context:\n",
    "asyncio.run(stop_team_externally())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75112c6a",
   "metadata": {},
   "source": [
    "## Resuming the Team\n",
    "\n",
    "Teams maintain state and can resume from where they left off.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3bb780a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial run complete.\n",
      "Team resumed and continued.\n"
     ]
    }
   ],
   "source": [
    "async def resume_team():\n",
    "    await team.reset()  # Reset first for demonstration\n",
    "    await team.run(task=\"Start a brainstorming session on renewable energy.\")\n",
    "    print(\"Initial run complete.\")\n",
    "\n",
    "    # Resume without a new task\n",
    "    await team.run()\n",
    "    print(\"Team resumed and continued.\")\n",
    "\n",
    "# To execute, run in an async context:\n",
    "asyncio.run(resume_team())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da428bf",
   "metadata": {},
   "source": [
    "## Aborting the Team\n",
    "\n",
    "Aborting immediately stops the team and raises a cancellation exception.\n",
    "\n",
    "Even though we catch the exception, Jupyter will still print it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8a98ca70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error processing publish message for analyst_a14046b4-f488-4270-a73a-0e67a09086f1/a14046b4-f488-4270-a73a-0e67a09086f1\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/autogen_core/_single_threaded_agent_runtime.py\", line 606, in _on_message\n",
      "    return await agent.on_message(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/autogen_core/_base_agent.py\", line 119, in on_message\n",
      "    return await self.on_message_impl(message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/autogen_agentchat/teams/_group_chat/_sequential_routed_agent.py\", line 67, in on_message_impl\n",
      "    return await super().on_message_impl(message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/autogen_core/_routed_agent.py\", line 485, in on_message_impl\n",
      "    return await h(self, message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/autogen_core/_routed_agent.py\", line 268, in wrapper\n",
      "    return_value = await func(self, message, ctx)  # type: ignore\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/autogen_agentchat/teams/_group_chat/_chat_agent_container.py\", line 133, in handle_request\n",
      "    async for msg in self._agent.on_messages_stream(self._message_buffer, ctx.cancellation_token):\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py\", line 953, in on_messages_stream\n",
      "    async for inference_output in self._call_llm(\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py\", line 1107, in _call_llm\n",
      "    model_result = await model_client.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/autogen_ext/models/openai/_openai_client.py\", line 691, in create\n",
      "    result: Union[ParsedChatCompletion[BaseModel], ChatCompletion] = await future\n",
      "                                                                     ^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
      "    yield self  # This tells Task to wait for completion.\n",
      "    ^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
      "    future.result()\n",
      "  File \"/opt/conda/lib/python3.12/asyncio/futures.py\", line 197, in result\n",
      "    raise self._make_cancelled_error()\n",
      "  File \"/opt/conda/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py\", line 2589, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/openai/_base_client.py\", line 1794, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/openai/_base_client.py\", line 1529, in request\n",
      "    response = await self._client.send(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/httpx/_client.py\", line 1629, in send\n",
      "    response = await self._send_handling_auth(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/httpx/_client.py\", line 1657, in _send_handling_auth\n",
      "    response = await self._send_handling_redirects(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/httpx/_client.py\", line 1694, in _send_handling_redirects\n",
      "    response = await self._send_single_request(request)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/httpx/_client.py\", line 1730, in _send_single_request\n",
      "    response = await transport.handle_async_request(request)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/httpx/_transports/default.py\", line 394, in handle_async_request\n",
      "    resp = await self._pool.handle_async_request(req)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/httpcore/_async/connection_pool.py\", line 256, in handle_async_request\n",
      "    raise exc from None\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/httpcore/_async/connection_pool.py\", line 236, in handle_async_request\n",
      "    response = await connection.handle_async_request(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/httpcore/_async/connection.py\", line 103, in handle_async_request\n",
      "    return await self._connection.handle_async_request(request)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/httpcore/_async/http11.py\", line 136, in handle_async_request\n",
      "    raise exc\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/httpcore/_async/http11.py\", line 106, in handle_async_request\n",
      "    ) = await self._receive_response_headers(**kwargs)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/httpcore/_async/http11.py\", line 177, in _receive_response_headers\n",
      "    event = await self._receive_event(timeout=timeout)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/httpcore/_async/http11.py\", line 217, in _receive_event\n",
      "    data = await self._network_stream.read(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/httpcore/_backends/anyio.py\", line 35, in read\n",
      "    return await self._stream.receive(max_bytes=max_bytes)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/anyio/streams/tls.py\", line 204, in receive\n",
      "    data = await self._call_sslobject_method(self._ssl_object.read, max_bytes)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/anyio/streams/tls.py\", line 147, in _call_sslobject_method\n",
      "    data = await self.transport_stream.receive()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 1246, in receive\n",
      "    await self._protocol.read_event.wait()\n",
      "  File \"/opt/conda/lib/python3.12/asyncio/locks.py\", line 212, in wait\n",
      "    await fut\n",
      "  File \"/opt/conda/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
      "    yield self  # This tells Task to wait for completion.\n",
      "    ^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
      "    future.result()\n",
      "  File \"/opt/conda/lib/python3.12/asyncio/futures.py\", line 197, in result\n",
      "    raise self._make_cancelled_error()\n",
      "asyncio.exceptions.CancelledError\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Team run was aborted.\n"
     ]
    }
   ],
   "source": [
    "from autogen_core import CancellationToken\n",
    "import asyncio\n",
    "\n",
    "async def abort_team():\n",
    "    cancellation_token = CancellationToken()\n",
    "    run_task = asyncio.create_task(\n",
    "        team.run(task=\"Generate a detailed report on climate change.\", cancellation_token=cancellation_token)\n",
    "    )\n",
    "    await asyncio.sleep(0.1)  # Let the team start\n",
    "    cancellation_token.cancel()  # Abort the team\n",
    "    try:\n",
    "        await run_task\n",
    "    except asyncio.exceptions.CancelledError:\n",
    "        print(\"Team run was aborted.\")\n",
    "\n",
    "# To execute, run in an async context:\n",
    "asyncio.run(abort_team())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adebeb98-dde8-4b15-9b35-4584f4db49d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
