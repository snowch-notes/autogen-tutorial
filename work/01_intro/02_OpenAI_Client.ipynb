{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea34f5f0-8312-47fe-8b75-53f36092ec2e",
   "metadata": {},
   "source": [
    "# OpenAIChatCompletionClient Tutorial\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this tutorial, you will understand:\n",
    "- How to make direct API calls to OpenAI using AutoGen's `OpenAIChatCompletionClient`\n",
    "- When to use direct API calls vs. AutoGen's AgentChat framework\n",
    "- Best practices for error handling\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The `OpenAIChatCompletionClient` [[api docs](https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.openai.html#autogen_ext.models.openai.OpenAIChatCompletionClient)] provides a direct interface to OpenAI's Chat Completion API. This is useful when you need:\n",
    "\n",
    "- **Simple, synchronous conversations** with a single AI model\n",
    "- **Direct control** over API parameters and response handling\n",
    "- **Integration** into existing applications without the full agent framework\n",
    "- **Testing and prototyping** before building more complex multi-agent systems\n",
    "\n",
    "For more sophisticated scenarios involving multiple agents, tool usage, or complex conversation flows, you'll want to use AutoGen's AgentChat framework (covered in the following tutorials)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0724b261-a664-4f0b-890e-9226efaa801f",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "### API Key Configuration\n",
    "\n",
    "Before starting, ensure you have an OpenAI API key. There are several ways to configure this:\n",
    "\n",
    "1. **Environment variable (recommended)**: We used a `.env` file with `OPENAI_API_KEY=your_key_here`\n",
    "2. **Direct parameter**: Pass the API key directly to the client (not recommended for production)\n",
    "3. **System environment**: Export the key in your shell session\n",
    "\n",
    "Let's verify the API key is available as an environment variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0c33af5-78aa-4028-989a-24dc1f18fcd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì API key found: sk-proj-####\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if api_key:\n",
    "    print(f\"‚úì API key found: {api_key[:8]}####\")\n",
    "else:\n",
    "    print(\"‚ùå OPENAI_API_KEY environment variable not found\")\n",
    "    print(\"Please set your API key before continuing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a921a0ce-3d2a-48e3-bf49-1cf35d171933",
   "metadata": {},
   "source": [
    "### Install Required Dependencies\n",
    "\n",
    "Install AutoGen with OpenAI support and Rich for better output formatting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff6ca59d-e348-427a-b323-ad7e41698315",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install --quiet -U \"autogen-ext[openai]>=0.7\" rich"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f01c6f-e35c-47e5-8aee-48e5bdaaccb9",
   "metadata": {},
   "source": [
    "## Basic Example - simple Question-Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cceef827-5d7c-4c89-824d-c1c702f00108",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">CreateResult</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">finish_reason</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'stop'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'The capital of France is Paris.'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">usage</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">RequestUsage</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">prompt_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15</span>, <span style=\"color: #808000; text-decoration-color: #808000\">completion_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span><span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">cached</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">logprobs</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">thought</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mCreateResult\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mfinish_reason\u001b[0m=\u001b[32m'stop'\u001b[0m,\n",
       "    \u001b[33mcontent\u001b[0m=\u001b[32m'The capital of France is Paris.'\u001b[0m,\n",
       "    \u001b[33musage\u001b[0m=\u001b[1;35mRequestUsage\u001b[0m\u001b[1m(\u001b[0m\u001b[33mprompt_tokens\u001b[0m=\u001b[1;36m15\u001b[0m, \u001b[33mcompletion_tokens\u001b[0m=\u001b[1;36m7\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[33mcached\u001b[0m=\u001b[3;91mFalse\u001b[0m,\n",
       "    \u001b[33mlogprobs\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "    \u001b[33mthought\u001b[0m=\u001b[3;35mNone\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rich import print as rprint\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "from autogen_core.models import UserMessage\n",
    "\n",
    "# OpenAIChatCompletionClient can retreive the api key from the OPENAI_API_KEY environment variable\n",
    "openai_client = OpenAIChatCompletionClient(\n",
    "    model=\"gpt-4o-2024-08-06\"\n",
    ")\n",
    "\n",
    "result = await openai_client.create([UserMessage(content=\"What is the capital of France?\", source=\"user\")])\n",
    "rprint(result)\n",
    "\n",
    "# Close the client when done.\n",
    "await openai_client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3281bc18-342a-42f9-b6b5-813d9fcffbfb",
   "metadata": {},
   "source": [
    "We'll focus on a few of the `CreateResult` attributes here.\n",
    "\n",
    "The `finish_reason` attribute contains the reason the model stopped generating text:\n",
    "\n",
    "- `stop` - Natural completion (good!)\n",
    "- `length` - Hit token limit (response may be cut off)\n",
    "- `content_filter` - Blocked by safety filters\n",
    "- `tool_calls` - Wants to use a function/tool\n",
    "- `unknown` - Unexpected termination (API error or interruption)\n",
    "\n",
    "The `content` attribute contains the actual AI response text - this is what you typically use.\n",
    "\n",
    "The `usage` attribute: Token consumption for billing and monitoring:\n",
    "\n",
    "- `prompt_tokens=15` - Your input tokens\n",
    "- `completion_tokens=7` - AI's response tokens\n",
    "- Total cost = based on both values\n",
    "\n",
    "**TIP:** It is highly recommended that you take a minute to explore the `CreateResult` [api docs](https://microsoft.github.io/autogen/stable/reference/python/autogen_core.models.html#autogen_core.models.CreateResult)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parameters-section",
   "metadata": {},
   "source": [
    "## Advanced Configuration\n",
    "\n",
    "### Controlling Response Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b6a4b6-4665-4399-ae89-b4caa2d76ec1",
   "metadata": {},
   "source": [
    "We can control the response by providing parameters to the `create()` call.  \n",
    "\n",
    "Here we use `json_output=True` [api docs](https://microsoft.github.io/autogen/stable/reference/python/autogen_core.models.html#autogen_core.models.ChatCompletionClient.create) to state that the response should be in json.\n",
    "\n",
    "In this example we haven't asked OpenAI to return Json so an exception is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "parameters-example",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received response:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">The capital of France is Paris.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "The capital of France is Paris.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Error code: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">400</span> - <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'error'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'message'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"'messages' must contain the word 'json' in some form, to use </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'response_format' of type 'json_object'.\"</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'invalid_request_error'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'param'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'messages'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'code'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span><span style=\"font-weight: bold\">}}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Error code: \u001b[1;36m400\u001b[0m - \u001b[1m{\u001b[0m\u001b[32m'error'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'message'\u001b[0m: \u001b[32m\"'messages' must contain the word 'json' in some form, to use \u001b[0m\n",
       "\u001b[32m'response_format' of type 'json_object'.\"\u001b[0m, \u001b[32m'type'\u001b[0m: \u001b[32m'invalid_request_error'\u001b[0m, \u001b[32m'param'\u001b[0m: \u001b[32m'messages'\u001b[0m, \u001b[32m'code'\u001b[0m: \u001b[3;35mNone\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rich import print as rprint\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "from autogen_core.models import UserMessage, SystemMessage\n",
    "\n",
    "# OpenAIChatCompletionClient can retreive the api key from the OPENAI_API_KEY environment variable\n",
    "openai_client = OpenAIChatCompletionClient(\n",
    "    model=\"gpt-4o-2024-08-06\"\n",
    ")\n",
    "\n",
    "try:\n",
    "    result = await openai_client.create(\n",
    "        [UserMessage(content=\"What is the capital of France?\", source=\"user\")],\n",
    "        json_output=True\n",
    "    )\n",
    "    rprint(result)\n",
    "except Exception as e:\n",
    "    print(f\"Received response:\")\n",
    "    rprint(result.content)\n",
    "    print(f\"Error:\")\n",
    "    rprint(e)\n",
    "\n",
    "# Close the client when done.\n",
    "await openai_client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9686a85-6d42-43cc-8148-4bc6c690e6b1",
   "metadata": {},
   "source": [
    "**NOTE**: In a later tutorial we will show you how to use a Pydantic model to provide robut data format quality checks for the LLM's output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "error-handling-section",
   "metadata": {},
   "source": [
    "## Error Handling and Best Practices\n",
    "\n",
    "In this example we should how error handling could be implemented to make your client interaction more robust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "error-handling-example",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Success: Hello! I'm here and ready to help. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from openai import RateLimitError, APIConnectionError, AuthenticationError\n",
    "\n",
    "async def robust_api_call(messages, max_retries=3):\n",
    "    \"\"\"Make an API call with proper error handling and retries.\"\"\"\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            client = OpenAIChatCompletionClient(\n",
    "                model=\"gpt-4o-2024-08-06\"\n",
    "            )\n",
    "                \n",
    "            result = await client.create(messages)\n",
    "            return result\n",
    "                \n",
    "        except AuthenticationError as e:\n",
    "            print(f\"‚ùå Authentication error: {e}\")\n",
    "            print(\"Please check your API key.\")\n",
    "            break  # Don't retry auth errors\n",
    "            \n",
    "        except RateLimitError as e:\n",
    "            wait_time = 2 ** attempt  # Exponential backoff\n",
    "            print(f\"‚è±Ô∏è Rate limit hit. Waiting {wait_time} seconds... (attempt {attempt + 1}/{max_retries})\")\n",
    "            await asyncio.sleep(wait_time)\n",
    "            \n",
    "        except APIConnectionError as e:\n",
    "            print(f\"üåê Connection error: {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                await asyncio.sleep(1)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Unexpected error: {type(e).__name__}: {e}\")\n",
    "            break\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Example usage\n",
    "async def error_handling_example():\n",
    "    messages = [UserMessage(content=\"Hello, how are you?\", source=\"user\")]\n",
    "    \n",
    "    result = await robust_api_call(messages)\n",
    "    \n",
    "    if result:\n",
    "        print(f\"‚úÖ Success: {result.content}\")\n",
    "    else:\n",
    "        print(\"‚ùå Failed to get response after retries\")\n",
    "\n",
    "await error_handling_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "troubleshooting-section",
   "metadata": {},
   "source": [
    "## Troubleshooting Common Issues\n",
    "\n",
    "### Issue: \"Authentication Error\"\n",
    "- **Cause**: Invalid or missing API key\n",
    "- **Solution**: Verify your `OPENAI_API_KEY` environment variable\n",
    "\n",
    "### Issue: \"Rate limit exceeded\"\n",
    "- **Cause**: Too many requests in a short time\n",
    "- **Solution**: Implement exponential backoff (see error handling example above)\n",
    "\n",
    "### Issue: \"Model not found\"\n",
    "- **Cause**: Invalid model name or no access to specified model\n",
    "- **Solution**: Check available models in your OpenAI account\n",
    "\n",
    "### Issue: High token usage\n",
    "- **Cause**: Long conversations or verbose responses\n",
    "- **Solution**: Use `max_tokens` parameter and manage conversation history length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next-steps-section",
   "metadata": {},
   "source": [
    "## Next Steps: Moving to AutoGen AutoChat Framework\n",
    "\n",
    "Now that you understand direct API calls, you're ready to explore AutoGen's more powerful features:\n",
    "\n",
    "- **Assistants**: Pre-configured agents with specific roles and capabilities\n",
    "- **Message routing**: Intelligent conversation management between multiple agents\n",
    "- **Tool integration**: Agents that can call functions and use external tools\n",
    "- **Conversation patterns**: Group chats, sequential conversations, and complex workflows\n",
    "\n",
    "The next tutorial will show you how to create assistants that can collaborate, use tools, and handle complex multi-step tasks that go far beyond simple API calls.\n",
    "\n",
    "**Key takeaway**: Use direct `autogen-core` API calls when you need simple, controlled interactions. Use the AutoGen `AgentChat` framework when you need sophisticated agent behaviors and multi-agent coordination."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "task-section",
   "metadata": {},
   "source": [
    "## Practice Exercises\n",
    "\n",
    "1. Try connecting to other [AI providers](https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/models.html) using AutoGen's unified interface, for example:\n",
    "\n",
    "- **Anthropic Claude**\n",
    "- **Google Gemini**\n",
    "- **Azure OpenAI**\n",
    "\n",
    "2. Experiment with different models (e.g. gpt-4o-mini) and compare their responses to the same prompts!\n",
    "\n",
    "3. Run the streaming example in the OpenAIChatCompletionClient [api docs](https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.openai.html#autogen_ext.models.openai.OpenAIChatCompletionClient)\n",
    "\n",
    "3. Explore the api doc links in this tutorial.  It is very important to become familiar with the api docs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f474b139-93c6-464e-939f-283684c0f885",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
