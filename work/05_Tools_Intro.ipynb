{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "tools-intro-overview",
   "metadata": {},
   "source": [
    "# Introduction to Tools in AutoGen AgentChat\n",
    "\n",
    "## Overview\n",
    "\n",
    "Language Models (LLMs) are powerful, but their capabilities are often limited to generating text. To enable agents to perform real-world actions, interact with external systems, or access up-to-date information, they need the ability to use **tools**.\n",
    "\n",
    "In AutoGen AgentChat, tools are functions or external services that an agent can call to extend its capabilities. This notebook introduces the fundamental concepts of tools, how to define them, and how to integrate them with an `AssistantAgent`.\n",
    "\n",
    "A tool is essentially a callable function or a subclass of `autogen_core.tools.BaseTool`. The `AssistantAgent` automatically converts Python functions into `FunctionTool` instances, generating the necessary schema from function signatures and docstrings. Crucially, when an `AssistantAgent` executes a tool, it does so directly within the same call to its `run()` method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prerequisites",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Ensure you have the necessary packages installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8528cca0-5543-41cc-af88-94a9e57396dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet -U \"autogen-agentchat>=0.7\" \"autogen-ext[openai]>=0.7\" rich\n",
    "\n",
    "# IMPORTANT: See: https://github.com/microsoft/autogen/issues/6906\n",
    "!pip install --quiet --force-reinstall \"openai==1.80\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4ad1c8-b5cc-4337-b46e-3547744c447f",
   "metadata": {},
   "source": [
    "> **âš ï¸ IMPORTANT:**  \n",
    "> If you just ran the `!pip install --quiet --force-reinstall \"openai==1.80\"` command,  \n",
    "> you **must** restart the Jupyter kernel before continuing.  \n",
    "> This ensures the newly installed `openai` package is loaded into memory  \n",
    "> and avoids mixed-version issues that can cause runtime errors.  \n",
    ">  \n",
    "> **In Jupyter:** go to **Kernel â†’ Restart & Clear Output**, then rerun the notebook from the top."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defining-a-simple-tool",
   "metadata": {},
   "source": [
    "## Defining a Simple Tool (Python Function)\n",
    "\n",
    "The simplest way to define a tool is by writing a standard Python function. AutoGen's `AssistantAgent` can automatically convert this function into a callable tool for the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "simple-tool-example",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”„ Task: Greet Alice.\n",
      "----------------------------------------\n",
      "---------- TextMessage (user) ----------\n",
      "Greet someone named Alice.\n",
      "---------- ToolCallRequestEvent (greeter) ----------\n",
      "[FunctionCall(id='call_bS8hidtLLcWSodIkswMUW4UR', arguments='{\"name\":\"Alice\"}', name='greet')]\n",
      "---------- ToolCallExecutionEvent (greeter) ----------\n",
      "[FunctionExecutionResult(content='Hello, Alice! Nice to meet you.', name='greet', call_id='call_bS8hidtLLcWSodIkswMUW4UR', is_error=False)]\n",
      "---------- ToolCallSummaryMessage (greeter) ----------\n",
      "Hello, Alice! Nice to meet you.\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from autogen_core.tools import FunctionTool\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "from autogen_agentchat.ui import Console\n",
    "\n",
    "# Define a tool using a Python function\n",
    "def greet(name: str) -> str:\n",
    "    \"\"\"Return a friendly greeting for the user.\"\"\"\n",
    "    return f\"Hello, {name}! Nice to meet you.\"\n",
    "\n",
    "# Wrap it into a FunctionTool (this step is often implicit with AssistantAgent)\n",
    "greet_tool = FunctionTool(greet, description=\"Return a friendly greeting for a given name.\")\n",
    "\n",
    "async def main():\n",
    "    # Create the OpenAI model client\n",
    "    model_client = OpenAIChatCompletionClient(model=\"gpt-4o-mini\")\n",
    "\n",
    "    # Create the agent, passing the tool\n",
    "    agent = AssistantAgent(\n",
    "        name=\"greeter\",\n",
    "        model_client=model_client,\n",
    "        tools=[greet_tool],\n",
    "        system_message=\"You are a helpful assistant that can greet people by name.\"\n",
    "    )\n",
    "\n",
    "    print(\"\\nðŸ”„ Task: Greet Alice.\")\n",
    "    print(\"-\" * 40)\n",
    "    # Ask the agent to use the tool\n",
    "    await Console(agent.run_stream(task=\"Greet someone named Alice.\"))\n",
    "\n",
    "    # Cleanup\n",
    "    await model_client.close()\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tool-output-reflection",
   "metadata": {},
   "source": [
    "## Tool Output Reflection (`reflect_on_tool_use`)\n",
    "\n",
    "By default, when an `AssistantAgent` executes a tool, it will return the tool's raw output as a string in a `ToolCallSummaryMessage`. However, if your tool's output is not a well-formed natural language string (e.g., it's a complex JSON object, a raw data dump, or just a boolean), you might want the model to *reflect* on the tool's output and summarize it in natural language.\n",
    "\n",
    "You can enable this reflection step by setting the `reflect_on_tool_use=True` parameter in the `AssistantAgent` constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "reflection-example",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”„ Task (with reflection): Report system status.\n",
      "----------------------------------------\n",
      "---------- TextMessage (user) ----------\n",
      "What is the current system status?\n",
      "---------- ToolCallRequestEvent (status_reporter) ----------\n",
      "[FunctionCall(id='call_QMb8zVffeVBIVjJDvPZTj91q', arguments='{}', name='get_system_status')]\n",
      "---------- ToolCallExecutionEvent (status_reporter) ----------\n",
      "[FunctionExecutionResult(content='{\"cpu_usage\": \"85%\", \"memory_free\": \"2GB\", \"service_status\": \"running\"}', name='get_system_status', call_id='call_QMb8zVffeVBIVjJDvPZTj91q', is_error=False)]\n",
      "---------- TextMessage (status_reporter) ----------\n",
      "The current system status is as follows:\n",
      "- CPU Usage: 85%\n",
      "- Free Memory: 2GB\n",
      "- Service Status: Running\n",
      "\n",
      "ðŸ”„ Task (without reflection): Report system status.\n",
      "----------------------------------------\n",
      "---------- TextMessage (user) ----------\n",
      "What is the current system status?\n",
      "---------- ToolCallRequestEvent (raw_reporter) ----------\n",
      "[FunctionCall(id='call_RNBqeDLcReSLmQ2mySRhuAMK', arguments='{}', name='get_system_status')]\n",
      "---------- ToolCallExecutionEvent (raw_reporter) ----------\n",
      "[FunctionExecutionResult(content='{\"cpu_usage\": \"85%\", \"memory_free\": \"2GB\", \"service_status\": \"running\"}', name='get_system_status', call_id='call_RNBqeDLcReSLmQ2mySRhuAMK', is_error=False)]\n",
      "---------- ToolCallSummaryMessage (raw_reporter) ----------\n",
      "{\"cpu_usage\": \"85%\", \"memory_free\": \"2GB\", \"service_status\": \"running\"}\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "from autogen_agentchat.ui import Console\n",
    "\n",
    "# A tool that returns a non-natural language output (e.g., JSON string)\n",
    "def get_system_status() -> str:\n",
    "    \"\"\"Returns the current system status as a JSON string.\"\"\"\n",
    "    return '{\"cpu_usage\": \"85%\", \"memory_free\": \"2GB\", \"service_status\": \"running\"}'\n",
    "\n",
    "async def run_reflection_example():\n",
    "    model_client = OpenAIChatCompletionClient(model=\"gpt-4o-mini\")\n",
    "\n",
    "    # Agent with reflection enabled\n",
    "    agent_with_reflection = AssistantAgent(\n",
    "        name=\"status_reporter\",\n",
    "        model_client=model_client,\n",
    "        tools=[get_system_status],\n",
    "        system_message=\"You are an assistant that reports system status. Summarize the tool output.\",\n",
    "        reflect_on_tool_use=True # Enable reflection\n",
    "    )\n",
    "\n",
    "    print(\"\\nðŸ”„ Task (with reflection): Report system status.\")\n",
    "    print(\"-\" * 40)\n",
    "    await Console(agent_with_reflection.run_stream(task=\"What is the current system status?\"))\n",
    "\n",
    "    # Agent without reflection (for comparison)\n",
    "    agent_no_reflection = AssistantAgent(\n",
    "        name=\"raw_reporter\",\n",
    "        model_client=model_client,\n",
    "        tools=[get_system_status],\n",
    "        system_message=\"You are an assistant that reports system status. Just output the raw tool result.\",\n",
    "        reflect_on_tool_use=False # Explicitly disable reflection\n",
    "    )\n",
    "\n",
    "    print(\"\\nðŸ”„ Task (without reflection): Report system status.\")\n",
    "    print(\"-\" * 40)\n",
    "    await Console(agent_no_reflection.run_stream(task=\"What is the current system status?\"))\n",
    "\n",
    "    await model_client.close()\n",
    "\n",
    "await run_reflection_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next-steps",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "This notebook covered the basics of defining and using tools, along with the `reflect_on_tool_use` parameter. To continue learning:\n",
    "\n",
    "1.  **Explore Built-in Tools**: Check out `08_Built_in_Tools.ipynb` for a variety of pre-built tools in AutoGen Extension.\n",
    "2.  **Advanced Tool Usage**: Dive into `10_Advanced_Tool_Usage.ipynb` to learn about parallel tool calls and tool iterations.\n",
    "3.  **Custom Tools**: Implement more complex custom tools that interact with external APIs or databases.\n",
    "4.  **Tool Validation**: Explore how to add input and output validation to your tools using Pydantic."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
