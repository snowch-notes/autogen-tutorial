{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "advanced-tool-usage-overview",
   "metadata": {},
   "source": [
    "# Advanced Tool Usage: Parallel Calls and Iterations\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook delves into advanced configurations for tool usage within AutoGen AgentChat, specifically focusing on:\n",
    "\n",
    "- **Parallel Tool Calls**: How agents can execute multiple tools concurrently when supported by the underlying language model.\n",
    "- **Tool Iterations**: Configuring agents to perform multiple rounds of tool calls until a task is complete or a maximum iteration limit is reached.\n",
    "\n",
    "These features provide greater flexibility and efficiency for agents tackling complex tasks that require multiple external interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prerequisites",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Ensure you have the necessary packages installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71cbecb5-4cdf-4da7-a890-4447e11f5b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet -U \"autogen-agentchat>=0.7\" \"autogen-ext[openai]>=0.7\" rich\n",
    "\n",
    "# IMPORTANT: See: https://github.com/microsoft/autogen/issues/6906\n",
    "!pip install --quiet --force-reinstall \"openai==1.80\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00c5e84-8f82-4373-8145-3e8330e9e19f",
   "metadata": {},
   "source": [
    "> **âš ï¸ IMPORTANT:**  \n",
    "> If you just ran the `!pip install --quiet --force-reinstall \"openai==1.80\"` command,  \n",
    "> you **must** restart the Jupyter kernel before continuing.  \n",
    "> This ensures the newly installed `openai` package is loaded into memory  \n",
    "> and avoids mixed-version issues that can cause runtime errors.  \n",
    ">  \n",
    "> **In Jupyter:** go to **Kernel â†’ Restart & Clear Output**, then rerun the notebook from the top."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parallel-tool-calls",
   "metadata": {},
   "source": [
    "## Parallel Tool Calls\n",
    "\n",
    "Some advanced language models (like OpenAI's GPT-4o) can generate multiple tool calls in a single response. By default, `AssistantAgent` will execute these tool calls in parallel.\n",
    "\n",
    "You might want to disable parallel tool calls if your tools have side effects that could interfere with each other, or if you need consistent behavior across models that don't support parallel calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "parallel-tool-calls-example",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”„ Task (Parallel Enabled): What is the weather and time in London and Paris?\n",
      "----------------------------------------\n",
      "---------- TextMessage (user) ----------\n",
      "What is the weather and time in London and Paris?\n",
      "---------- ToolCallRequestEvent (parallel_assistant) ----------\n",
      "[FunctionCall(id='call_cMsdlMuPHU6z25jayzDKM4VW', arguments='{\"city\": \"London\"}', name='get_weather'), FunctionCall(id='call_wVHQoli0rD6ONZkXrh90kDRZ', arguments='{\"city\": \"London\"}', name='get_time'), FunctionCall(id='call_zMM8aGMxg9rxpAAf33k0rxdx', arguments='{\"city\": \"Paris\"}', name='get_weather'), FunctionCall(id='call_D6zznw1JvoassWGOizpUQOoe', arguments='{\"city\": \"Paris\"}', name='get_time')]\n",
      "---------- ToolCallExecutionEvent (parallel_assistant) ----------\n",
      "[FunctionExecutionResult(content='The weather in London is cloudy with a temperature of 15Â°C.', name='get_weather', call_id='call_cMsdlMuPHU6z25jayzDKM4VW', is_error=False), FunctionExecutionResult(content='The current time in London is 10:00 AM GMT.', name='get_time', call_id='call_wVHQoli0rD6ONZkXrh90kDRZ', is_error=False), FunctionExecutionResult(content='The weather in Paris is sunny with a temperature of 22Â°C.', name='get_weather', call_id='call_zMM8aGMxg9rxpAAf33k0rxdx', is_error=False), FunctionExecutionResult(content='The current time in Paris is 11:00 AM CEST.', name='get_time', call_id='call_D6zznw1JvoassWGOizpUQOoe', is_error=False)]\n",
      "---------- ToolCallSummaryMessage (parallel_assistant) ----------\n",
      "The weather in London is cloudy with a temperature of 15Â°C.\n",
      "The current time in London is 10:00 AM GMT.\n",
      "The weather in Paris is sunny with a temperature of 22Â°C.\n",
      "The current time in Paris is 11:00 AM CEST.\n",
      "\n",
      "ðŸ”„ Task (Parallel Disabled): What is the weather and time in London and Paris?\n",
      "----------------------------------------\n",
      "---------- TextMessage (user) ----------\n",
      "What is the weather and time in London and Paris?\n",
      "---------- ToolCallRequestEvent (sequential_assistant) ----------\n",
      "[FunctionCall(id='call_4rKndtaDcmY6qO2ye1whwqQn', arguments='{\"city\":\"London\"}', name='get_weather')]\n",
      "---------- ToolCallExecutionEvent (sequential_assistant) ----------\n",
      "[FunctionExecutionResult(content='The weather in London is cloudy with a temperature of 15Â°C.', name='get_weather', call_id='call_4rKndtaDcmY6qO2ye1whwqQn', is_error=False)]\n",
      "---------- ToolCallSummaryMessage (sequential_assistant) ----------\n",
      "The weather in London is cloudy with a temperature of 15Â°C.\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "from autogen_agentchat.ui import Console\n",
    "\n",
    "# Define two mock tools\n",
    "async def get_weather(city: str) -> str:\n",
    "    \"\"\"Gets the current weather for a specified city.\"\"\"\n",
    "    await asyncio.sleep(1) # Simulate network delay\n",
    "    if city.lower() == \"london\":\n",
    "        return \"The weather in London is cloudy with a temperature of 15Â°C.\"\n",
    "    elif city.lower() == \"paris\":\n",
    "        return \"The weather in Paris is sunny with a temperature of 22Â°C.\"\n",
    "    return f\"Could not find weather for {city}.\"\n",
    "\n",
    "async def get_time(city: str) -> str:\n",
    "    \"\"\"Gets the current time for a specified city.\"\"\"\n",
    "    await asyncio.sleep(0.5) # Simulate network delay\n",
    "    if city.lower() == \"london\":\n",
    "        return \"The current time in London is 10:00 AM GMT.\"\n",
    "    elif city.lower() == \"paris\":\n",
    "        return \"The current time in Paris is 11:00 AM CEST.\"\n",
    "    return f\"Could not find time for {city}.\"\n",
    "\n",
    "async def run_parallel_tool_example():\n",
    "    # Agent with parallel tool calls enabled (default for supporting models)\n",
    "    model_client_parallel = OpenAIChatCompletionClient(model=\"gpt-4o-mini\")\n",
    "    agent_parallel = AssistantAgent(\n",
    "        name=\"parallel_assistant\",\n",
    "        model_client=model_client_parallel,\n",
    "        tools=[get_weather, get_time],\n",
    "        system_message=\"You are an assistant that can get weather and time for cities. Use both tools if needed.\"\n",
    "    )\n",
    "\n",
    "    print(\"\\nðŸ”„ Task (Parallel Enabled): What is the weather and time in London and Paris?\")\n",
    "    print(\"-\" * 40)\n",
    "    await Console(agent_parallel.run_stream(task=\"What is the weather and time in London and Paris?\"))\n",
    "\n",
    "    # Agent with parallel tool calls disabled\n",
    "    model_client_no_parallel = OpenAIChatCompletionClient(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        parallel_tool_calls=False # Explicitly disable parallel calls\n",
    "    )\n",
    "    agent_no_parallel = AssistantAgent(\n",
    "        name=\"sequential_assistant\",\n",
    "        model_client=model_client_no_parallel,\n",
    "        tools=[get_weather, get_time],\n",
    "        system_message=\"You are an assistant that can get weather and time for cities. Use both tools if needed.\"\n",
    "    )\n",
    "\n",
    "    print(\"\\nðŸ”„ Task (Parallel Disabled): What is the weather and time in London and Paris?\")\n",
    "    print(\"-\" * 40)\n",
    "    await Console(agent_no_parallel.run_stream(task=\"What is the weather and time in London and Paris?\"))\n",
    "\n",
    "    await model_client_parallel.close()\n",
    "    await model_client_no_parallel.close()\n",
    "\n",
    "await run_parallel_tool_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tool-iterations",
   "metadata": {},
   "source": [
    "## Tool Iterations\n",
    "\n",
    "By default, an `AssistantAgent` performs at most one tool iteration (one model call followed by one or more parallel tool calls). For tasks requiring multiple steps of tool interaction, you can configure the agent to execute multiple iterations using the `max_tool_iterations` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "tool-iterations-example",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”„ Task: Process all data steps.\n",
      "----------------------------------------\n",
      "---------- TextMessage (user) ----------\n",
      "Start data processing.\n",
      "---------- ToolCallRequestEvent (iterative_assistant) ----------\n",
      "[FunctionCall(id='call_eWznPrWKuQNgfEQCtP5HkFMD', arguments='{\"step\":1}', name='get_next_step_data')]\n",
      "---------- ToolCallExecutionEvent (iterative_assistant) ----------\n",
      "[FunctionExecutionResult(content='Initial data: User wants to process file A.', name='get_next_step_data', call_id='call_eWznPrWKuQNgfEQCtP5HkFMD', is_error=False)]\n",
      "---------- ToolCallRequestEvent (iterative_assistant) ----------\n",
      "[FunctionCall(id='call_j8t4C9OscftWS75b27glbLRB', arguments='{\"step\":2}', name='get_next_step_data')]\n",
      "---------- ToolCallExecutionEvent (iterative_assistant) ----------\n",
      "[FunctionExecutionResult(content='Processing file A complete. Next, process file B.', name='get_next_step_data', call_id='call_j8t4C9OscftWS75b27glbLRB', is_error=False)]\n",
      "---------- ToolCallRequestEvent (iterative_assistant) ----------\n",
      "[FunctionCall(id='call_vaNfm2Q8Qu4x3MZssPRW8y6M', arguments='{\"step\":3}', name='get_next_step_data')]\n",
      "---------- ToolCallExecutionEvent (iterative_assistant) ----------\n",
      "[FunctionExecutionResult(content='Processing file B complete. All files processed.', name='get_next_step_data', call_id='call_vaNfm2Q8Qu4x3MZssPRW8y6M', is_error=False)]\n",
      "---------- TextMessage (iterative_assistant) ----------\n",
      "All files have been successfully processed. The final status is complete.\n",
      "\n",
      "Total tool calls made: 3\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "from autogen_agentchat.ui import Console\n",
    "\n",
    "# A mock tool that simulates a multi-step process\n",
    "call_count = 0\n",
    "async def get_next_step_data(step: int) -> str:\n",
    "    \"\"\"Fetches data for a specific step in a multi-step process.\"\"\"\n",
    "    global call_count\n",
    "    call_count += 1\n",
    "    if step == 1:\n",
    "        return \"Initial data: User wants to process file A.\"\n",
    "    elif step == 2:\n",
    "        return \"Processing file A complete. Next, process file B.\"\n",
    "    elif step == 3:\n",
    "        return \"Processing file B complete. All files processed.\"\n",
    "    return \"No more steps.\"\n",
    "\n",
    "async def run_tool_iterations_example():\n",
    "    global call_count\n",
    "    call_count = 0 # Reset for demonstration\n",
    "\n",
    "    model_client = OpenAIChatCompletionClient(model=\"gpt-4o-mini\")\n",
    "    agent_loop = AssistantAgent(\n",
    "        name=\"iterative_assistant\",\n",
    "        model_client=model_client,\n",
    "        tools=[get_next_step_data],\n",
    "        system_message=(\n",
    "            \"You are an assistant that processes data in multiple steps. \"\n",
    "            \"Call 'get_next_step_data' with the current step number (starting from 1). \"\n",
    "            \"Continue calling the tool until all steps are complete. \"\n",
    "            \"Report the final status when done.\"\n",
    "        ),\n",
    "        max_tool_iterations=5 # Allow up to 5 tool calls in a loop\n",
    "    )\n",
    "\n",
    "    print(\"\\nðŸ”„ Task: Process all data steps.\")\n",
    "    print(\"-\" * 40)\n",
    "    await Console(agent_loop.run_stream(task=\"Start data processing.\"))\n",
    "\n",
    "    print(f\"\\nTotal tool calls made: {call_count}\")\n",
    "    await model_client.close()\n",
    "\n",
    "await run_tool_iterations_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next-steps",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Experiment further with advanced tool usage:\n",
    "\n",
    "1.  **Complex Parallel Scenarios**: Design a scenario where parallel tool calls are genuinely beneficial (e.g., fetching data from multiple independent APIs).\n",
    "2.  **Conditional Iterations**: Implement a tool that returns a specific signal (e.g., a boolean flag) to tell the agent when to stop iterating, rather than relying solely on `max_tool_iterations`.\n",
    "3.  **Error Handling**: Explore how agents handle errors during tool execution in iterative or parallel scenarios."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
