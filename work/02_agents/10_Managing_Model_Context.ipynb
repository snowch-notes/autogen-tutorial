{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "managing-model-context-overview",
   "metadata": {},
   "source": [
    "# Managing Model Context in AutoGen AgentChat\n",
    "\n",
    "## Overview\n",
    "\n",
    "Effective management of conversation history (model context) is crucial for building efficient and performant AI agents. Sending the entire conversation history to the Language Model (LLM) in every turn can lead to increased costs, slower responses, and hitting token limits.\n",
    "\n",
    "AutoGen AgentChat provides the `model_context` parameter in `AssistantAgent` to control how conversation history is managed. This notebook explores different strategies for managing model context, including:\n",
    "\n",
    "- **`UnboundedChatCompletionContext` (Default)**: Sends the full conversation history.\n",
    "- **`BufferedChatCompletionContext`**: Limits context to the last `n` messages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prerequisites",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Ensure you have the necessary packages installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49c66f60-0ee8-45a5-abfc-caa97e4466fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet -U \"autogen-agentchat>=0.7\" \"autogen-ext[openai]>=0.7\" rich"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unbounded-context",
   "metadata": {},
   "source": [
    "## Unbounded Chat Completion Context (Default)\n",
    "\n",
    "By default, `AssistantAgent` uses `UnboundedChatCompletionContext`, which means the entire conversation history is sent to the model with each new message. This is simple but can be inefficient for long conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "unbounded-context-example",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”„ Conversation with Unbounded Context:\n",
      "----------------------------------------\n",
      "---------- TextMessage (user) ----------\n",
      "What is the capital of France?\n",
      "---------- TextMessage (unbounded_agent) ----------\n",
      "The capital of France is Paris.\n",
      "---------- TextMessage (user) ----------\n",
      "And what about Germany?\n",
      "---------- TextMessage (unbounded_agent) ----------\n",
      "The capital of Germany is Berlin.\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "from autogen_agentchat.ui import Console\n",
    "from autogen_core.model_context import UnboundedChatCompletionContext\n",
    "\n",
    "async def run_unbounded_context_example():\n",
    "    model_client = OpenAIChatCompletionClient(model=\"gpt-4o-mini\")\n",
    "    agent = AssistantAgent(\n",
    "        name=\"unbounded_agent\",\n",
    "        model_client=model_client,\n",
    "        system_message=\"You are a helpful assistant.\",\n",
    "        model_context=UnboundedChatCompletionContext() # Explicitly set, though it's the default\n",
    "    )\n",
    "\n",
    "    print(\"\\nðŸ”„ Conversation with Unbounded Context:\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    # First turn\n",
    "    await Console(agent.run_stream(task=\"What is the capital of France?\"))\n",
    "\n",
    "    # Second turn - model will see previous message\n",
    "    await Console(agent.run_stream(task=\"And what about Germany?\"))\n",
    "\n",
    "    await model_client.close()\n",
    "\n",
    "await run_unbounded_context_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "buffered-context",
   "metadata": {},
   "source": [
    "## Buffered Chat Completion Context\n",
    "\n",
    "The `BufferedChatCompletionContext` allows you to limit the number of recent messages sent to the LLM. This is useful for controlling context length and reducing costs, especially in long-running conversations where older messages might not be relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "buffered-context-example",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”„ Conversation with Buffered Context (buffer_size=1):\n",
      "----------------------------------------\n",
      "---------- TextMessage (user) ----------\n",
      "What is the capital of France?\n",
      "---------- TextMessage (buffered_agent) ----------\n",
      "The capital of France is Paris.\n",
      "---------- TextMessage (user) ----------\n",
      "And what about Germany?\n",
      "---------- TextMessage (buffered_agent) ----------\n",
      "Could you please provide more context or specify what aspect of Germany you would like to know about? For example, are you interested in its history, culture, economy, politics, travel information, or something else?\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "from autogen_agentchat.ui import Console\n",
    "from autogen_core.model_context import BufferedChatCompletionContext\n",
    "\n",
    "async def run_buffered_context_example():\n",
    "    model_client = OpenAIChatCompletionClient(model=\"gpt-4o-mini\")\n",
    "    agent = AssistantAgent(\n",
    "        name=\"buffered_agent\",\n",
    "        model_client=model_client,\n",
    "        system_message=\"You are a helpful assistant.\",\n",
    "        model_context=BufferedChatCompletionContext(buffer_size=1) # Only keep the last message\n",
    "    )\n",
    "\n",
    "    print(\"\\nðŸ”„ Conversation with Buffered Context (buffer_size=1):\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    # First turn\n",
    "    await Console(agent.run_stream(task=\"What is the capital of France?\"))\n",
    "\n",
    "    # Second turn - model will NOT see the first message, only the system message and the current user message\n",
    "    await Console(agent.run_stream(task=\"And what about Germany?\"))\n",
    "\n",
    "    await model_client.close()\n",
    "\n",
    "await run_buffered_context_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next-steps",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Experiment further with model context management:\n",
    "\n",
    "1.  **Adjust `buffer_size`**: Observe how different values impact the conversation flow and model responses.\n",
    "2.  **Longer Conversations**: Test with very long conversations to see the effects of context truncation.\n",
    "3.  **Cost Optimization**: Consider how these strategies can help reduce API costs for production applications.\n",
    "4.  **Custom Context Management**: For advanced scenarios, explore creating your own custom `BaseChatCompletionContext` subclasses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57c4d39-6174-4c14-9180-3b2c4ca91053",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
