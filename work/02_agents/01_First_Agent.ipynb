{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bef0bbc6-d981-4896-a0e7-ce9080e55c1b",
   "metadata": {},
   "source": [
    "# AutoGen AutoChat Agent Tutorial\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this section, we explore AutoGen AutoChat, a powerful framework for building conversational AI agents that can interact with users and perform various tasks.\n",
    "\n",
    "**AutoChat** is a component of the AutoGen framework that enables the creation of intelligent chat agents capable of:\n",
    "- Processing natural language queries\n",
    "- Executing complex tasks through conversation\n",
    "- Maintaining context across interactions\n",
    "- Integrating with various language models\n",
    "\n",
    "We use an `autogen_agentchat.agents.AssistantAgent` to explore the core concepts of agents and demonstrate how they handle message processing and task execution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "key-concepts-section",
   "metadata": {},
   "source": [
    "## Key Concepts\n",
    "\n",
    "### Core Components\n",
    "\n",
    "- **AssistantAgent**: The primary agent class that handles user interactions and task execution\n",
    "- **TextMessage**: The fundamental message type for text-based communication\n",
    "- **TaskResult**: Contains the complete conversation history and results from agent execution\n",
    "- **Response**: Lower-level response object returned by the `on_messages` method\n",
    "\n",
    "### Message Flow\n",
    "\n",
    "The typical conversation flow follows this pattern:\n",
    "1. User sends a message/task → `TextMessage` with `source='user'`\n",
    "2. Assistant processes and responds → `TextMessage` with `source='assistant'`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f14549b-b632-4c08-ade0-a61cec1476ef",
   "metadata": {},
   "source": [
    "## Examples\n",
    "\n",
    "### Example 1: Simple Question-Answer\n",
    "\n",
    "This example demonstrates the most straightforward way to interact with an AssistantAgent using the high-level `run()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d152d4c4-3934-4efa-9791-83fd0e366b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install --quiet -U \"autogen-agentchat>=0.7\" \"autogen-ext[openai]>=0.7\" rich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0911c3a9-cb87-4e76-a32c-b9695ee47d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich import print as rprint\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "\n",
    "async def main() -> None:\n",
    "    # Initialize the model client\n",
    "    model_client = OpenAIChatCompletionClient(model=\"gpt-4o\")\n",
    "\n",
    "    # Create an AssistantAgent\n",
    "    agent = AssistantAgent(\"assistant\", model_client=model_client)\n",
    "    \n",
    "    # Execute a task using the high-level run() method\n",
    "    task_result = await agent.run(task=\"Say 'Hello World!' in five languages.\")\n",
    "    \n",
    "    # Display the results\n",
    "    rprint(task_result)\n",
    "    \n",
    "    # Clean up resources\n",
    "    await model_client.close()\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7a7f51-cbdd-4de3-8788-85d5623b04ae",
   "metadata": {},
   "source": [
    "#### Understanding the Output\n",
    "\n",
    "The `run()` method returns a `TaskResult` object containing:\n",
    "\n",
    "- **messages**: A list of `TextMessage` objects representing the conversation\n",
    "  - First message (`source='user'`): The initial task/prompt\n",
    "  - Second message (`source='assistant'`): The agent's response\n",
    "- **stop_reason**: Information about why the conversation ended (if applicable)\n",
    "\n",
    "#### Key Components Explained\n",
    "\n",
    "- **TaskResult** [[API docs](https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.base.html#autogen_agentchat.base.TaskResult)]: Container for the complete interaction results\n",
    "- **TextMessage** [[API docs](https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.messages.html#autogen_agentchat.messages.TextMessage)]: Individual message in the conversation\n",
    "\n",
    "#### Message Attributes\n",
    "\n",
    "Each `TextMessage` contains:\n",
    "- `id`: Unique identifier for the message\n",
    "- `source`: Origin of the message (`'user'` or `'assistant'`)\n",
    "- `content`: The actual message text\n",
    "- `models_usage`: Token usage information (for assistant messages)\n",
    "- `created_at`: Timestamp of message creation\n",
    "- `metadata`: Additional message metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d48e97-d223-40ef-9134-804bca2d034e",
   "metadata": {},
   "source": [
    "### Message Source Pattern\n",
    "\n",
    "Observe the `source` attribute for the messages:\n",
    "\n",
    "- **First TextMessage (source='user')**: This represents the initial task/prompt that was given to the agent. When you call `await agent.run(task=\"Say 'Hello World!' in five languages.\")`, AutoGen creates a TextMessage with the task content and marks it as coming from the 'user' source.\n",
    "- **Second TextMessage (source='assistant')**: This represents the response generated by the AssistantAgent. The agent processes the user's task and generates a response, which becomes a TextMessage with source='assistant'.\n",
    "\n",
    "This pattern follows the typical conversational flow:\n",
    "- User sends a message/task → TextMessage with source='user'\n",
    "- Assistant responds → TextMessage with source='assistant'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d772037-fded-412f-b35d-d7cd9291f153",
   "metadata": {},
   "source": [
    "### Example 2: Lower-Level API Usage\n",
    "\n",
    "This example shows how to use the lower-level `on_messages()` method for more granular control over message handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efc8489-5585-4739-899f-4b8395e86067",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich import print as rprint\n",
    "from autogen_core import CancellationToken\n",
    "from autogen_agentchat.messages import TextMessage\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "\n",
    "async def main() -> None:\n",
    "    # Initialize the model client\n",
    "    model_client = OpenAIChatCompletionClient(model=\"gpt-4o\")\n",
    "\n",
    "    # Create an AssistantAgent\n",
    "    agent = AssistantAgent(\"assistant\", model_client=model_client)\n",
    "    \n",
    "    # Create a message manually\n",
    "    user_message = TextMessage(\n",
    "        content=\"Say 'Hello World!' in five languages.\", \n",
    "        source=\"user\"\n",
    "    )\n",
    "    \n",
    "    # Process the message using the lower-level API\n",
    "    response = await agent.on_messages([user_message], CancellationToken())\n",
    "    \n",
    "    # Display the response\n",
    "    rprint(response)\n",
    "    \n",
    "    # Clean up resources\n",
    "    await model_client.close()\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d17db7-5959-44df-9207-87ef6ab4e6c8",
   "metadata": {},
   "source": [
    "#### Key Differences from High-Level API\n",
    "\n",
    "1. **Manual Message Creation**: You must create `TextMessage` objects explicitly\n",
    "2. **Response vs TaskResult**: Returns a `Response` object instead of `TaskResult`\n",
    "3. **Single Response**: Only returns the agent's response, not the full conversation history\n",
    "4. **Cancellation Support**: Accepts a `CancellationToken` for operation cancellation\n",
    "\n",
    "#### Understanding the Response Object\n",
    "\n",
    "The `Response` object contains:\n",
    "- **chat_message**: The single `TextMessage` response from the assistant\n",
    "- **inner_messages**: List of internal messages (empty for simple interactions)\n",
    "\n",
    "#### Frequently Asked Questions\n",
    "\n",
    "**Q: Why is there only one chat_message?**\n",
    "A: Unlike `run()` which returns the full conversation, `on_messages()` returns only the agent's response to your input messages.\n",
    "\n",
    "**Q: What are inner_messages? Why is it empty?**\n",
    "A: Inner messages contain internal processing steps or tool calls that occurred during response generation. For simple text responses, this list is typically empty."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "method-comparison-section",
   "metadata": {},
   "source": [
    "## Method Comparison\n",
    "\n",
    "| Feature | `run()` | `on_messages()` | `on_messages_stream()` |\n",
    "|---------|---------|-----------------|------------------------|\n",
    "| **Ease of Use** | High-level, simple | Lower-level, more control | Lower-level, streaming |\n",
    "| **Input** | String task | List of TextMessage objects | List of TextMessage objects |\n",
    "| **Output** | TaskResult with full conversation | Response with single message | AsyncIterator of Response objects |\n",
    "| **Use Case** | Quick tasks, simple interactions | Complex flows, message manipulation | Real-time streaming, long responses |\n",
    "| **Cancellation** | Not directly supported | Supports CancellationToken | Supports CancellationToken |\n",
    "| **Response Type** | Complete response at once | Complete response at once | Incremental streaming chunks |\n",
    "\n",
    "### Note on `on_messages_stream()`\n",
    "\n",
    "**`on_messages_stream()`** is particularly useful when:\n",
    "- You need to display responses as they're being generated (real-time UI updates)\n",
    "- Working with long responses that benefit from progressive display\n",
    "- Building interactive chat interfaces where users see typing indicators\n",
    "- Processing very long outputs where you want to start handling partial responses immediately\n",
    "- Implementing token-by-token or chunk-by-chunk processing workflows\n",
    "\n",
    "The streaming method returns an `AsyncIterator` that yields `Response` objects incrementally, allowing you to process and display content as it becomes available rather than waiting for the complete response.\n",
    "\n",
    "This would fit well in your best practices section as well, helping users understand when streaming is beneficial versus the standard synchronous approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "best-practices-section",
   "metadata": {},
   "source": [
    "## Best Practices\n",
    "\n",
    "### When to Use Each Method\n",
    "\n",
    "- **Use `run()`** for:\n",
    "  - Simple, one-off tasks\n",
    "  - Quick prototyping\n",
    "  - When you need the full conversation history\n",
    "  - Batch processing where you can wait for complete responses\n",
    "\n",
    "- **Use `on_messages()`** for:\n",
    "  - Building complex conversational flows\n",
    "  - When you need fine-grained control over messages\n",
    "  - Implementing custom message handling logic\n",
    "  - When cancellation support is required\n",
    "  - Processing responses that don't require real-time feedback\n",
    "\n",
    "- **Use `on_messages_stream()`** for:\n",
    "  - Interactive chat interfaces where users expect real-time responses\n",
    "  - Long-form content generation (stories, articles, code) where progressive display improves UX\n",
    "  - Applications requiring immediate feedback or \"typing\" indicators\n",
    "  - Processing very large responses where you want to start handling partial content\n",
    "  - Token-by-token analysis or real-time content filtering\n",
    "  - Building responsive UI experiences that feel more natural and engaging\n",
    "  - Scenarios where network latency benefits from chunked delivery\n",
    "  - Applications that need to provide user interruption capabilities during generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resource-management-section",
   "metadata": {},
   "source": [
    "### Always remember to close the model client to free up resources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resource-management-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of proper resource management\n",
    "async def example_with_cleanup():\n",
    "    model_client = OpenAIChatCompletionClient(model=\"gpt-4o\")\n",
    "    try:\n",
    "        agent = AssistantAgent(\"assistant\", model_client=model_client)\n",
    "        task_result = await agent.run(task=\"Your task here\")\n",
    "        return task_result\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {e}\")\n",
    "        # Handle error appropriately\n",
    "        raise\n",
    "    finally:\n",
    "        await model_client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next-steps-section",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "This tutorial covered the basics of AutoGen AutoChat agents. To continue learning:\n",
    "\n",
    "1. **Explore multi-agent conversations**: Learn how agents can communicate with each other\n",
    "2. **Learn about tool integration**: Discover how to give agents access to external tools and APIs\n",
    "3. **Implement custom agent behaviors**: Create specialized agents for specific use cases\n",
    "4. **Study advanced message types**: Explore beyond TextMessage for richer interactions\n",
    "\n",
    "For more advanced topics, refer to the [AutoGen documentation](https://microsoft.github.io/autogen/) and explore the various agent types and capabilities available in the framework.\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "- [AutoGen GitHub Repository](https://github.com/microsoft/autogen)\n",
    "- [API Reference Documentation](https://microsoft.github.io/autogen/stable/reference/python/)\n",
    "- [Community Examples and Tutorials](https://microsoft.github.io/autogen/stable/user-guide/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
